import json
import logging
import time
from datetime import datetime
from typing import List, Dict, Any

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI

from core.enums.source_type import SourceTypeEnum
from core.enums.video_category import VideoCategory
from core.llm.prompt_template_manager import PromptTemplateManager
from domain.channel.model.channel import Channel
from domain.comment.model.comment_type import CommentType
from domain.content_chunk.repository.content_chunk_repository import ContentChunkRepository
from domain.idea.dto.idea_dto import IdeaRequest
from domain.log.model.report_log import ReportLog
from domain.report.model.report import Report
from domain.trend_keyword.model.trend_keyword import TrendKeyword
from external.rag.rag_service import RagService
from external.youtube.transcript_service import TranscriptService
from external.youtube.trend_service import TrendService
from external.youtube.video_detail_service import VideoDetailService
from external.youtube.youtube_comment_service import YoutubeCommentService
from external.youtube.youtube_video_service import VideoService

logger = logging.getLogger(__name__)


class RagServiceImpl(RagService):
    def __init__(self):
        self.transcript_service = TranscriptService()
        self.video_detail_service = VideoDetailService()
        self.youtube_comment_service = YoutubeCommentService()
        self.content_chunk_repository = ContentChunkRepository()
        self.trend_service = TrendService()
        self.youtube_video_service = VideoService()
        self.llm = ChatOpenAI(model="gpt-4o-mini")
    
    async def summarize_video(self, video_id: str) -> str:
        context = await self.transcript_service.get_formatted_transcript(video_id)
        print("ì •ë¦¬ëœ ìë§‰ = ", context[:100])
        print()


        # ìë§‰ì´ ì—†ëŠ” ê²½ìš° ë°”ë¡œ ë©”ì‹œì§€ ë°˜í™˜
        if not context or context.strip() == "":
            return "ìë§‰ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ëŠ” ì˜ìƒì…ë‹ˆë‹¤."

        query = "ìœ íŠœë¸Œ ì˜ìƒ ìë§‰ì„ ê¸°ë°˜ìœ¼ë¡œ 10ì´ˆ ë‹¨ìœ„ ê°œìš”ë¥¼ ìœ„ì˜ í˜•ì‹ì— ë”°ë¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
        return self.execute_llm_chain(context, query, PromptTemplateManager.get_video_summary_prompt())
    
    def classify_comment(self, comment: str) -> Dict[str, Any]:
        query = "ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ ê°ì •ì„ ë¶„ë¥˜í•˜ê³  ë°±í‹±(```)ì´ë‚˜ ì„¤ëª… ì—†ì´ ìˆœìˆ˜ JSONìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”."
        result = self.execute_llm_chain(comment, query, PromptTemplateManager.get_comment_reaction_prompt())
        print("LLM ì‘ë‹µ = ", result)

        try:
            clean_json_str = result.strip().replace("```json", "").replace("```", "")
            result_json = json.loads(clean_json_str)
            
            # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ëœ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
            if isinstance(result_json, list):
                if result_json and isinstance(result_json[0], dict):
                    emotion_value = result_json[0].get("emotion")
                    logger.warning(f"LLMì´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨. ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©: {emotion_value}")
                else:
                    emotion_value = None
            else:
                emotion_value = result_json.get("emotion")
            
            return {
                "comment_type": CommentType.from_emotion_code(emotion_value)
            }
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}, ì›ë³¸ ì‘ë‹µ: {result}")
            return {
                "comment_type": CommentType.NEUTRAL
            }
        except Exception as e:
            logger.error(f"ëŒ“ê¸€ ë¶„ë¥˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}, ì‘ë‹µ: {result}")
            return {
                "comment_type": CommentType.NEUTRAL
            }

    def summarize_comments(self, comments: str) -> List[str]:
        query = (
            "ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½í•˜ê³  "
            "ë°±í‹±(```)ì´ë‚˜ ì„¤ëª… ì—†ì´ ìˆœìˆ˜ JSONìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”."
        )

        result = self.execute_llm_chain(
            comments, query, PromptTemplateManager.get_sumarlize_comment_prompt()
        )
        print("LLM ì‘ë‹µ = ", result)

        try:
            clean_json_str = result.strip().replace("```json", "").replace("```", "")
            result_list = json.loads(clean_json_str)
            contents = [item["content"] for item in result_list if isinstance(item, dict) and "content" in item]
            return contents
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}, ì›ë³¸ ì‘ë‹µ: {result}")
            return ["ëŒ“ê¸€ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

    async def get_popular_videos(self, category: VideoCategory):
        api_start = time.time()
        logger.info(f"{category.name} YouTube ì¸ê¸° ë™ì˜ìƒ API í˜¸ì¶œ ì¤‘...")

        # 2. ì¸ê¸° ë™ì˜ìƒ ëª©ë¡ ìœ íŠœë¸Œ í˜¸ì¶œ (YouTube API)
        category_id = category.value
        popular_videos = self.youtube_video_service.get_category_popular(category_id)

        api_time = time.time() - api_start
        logger.info(f"ğŸ“± YouTube ì¸ê¸° ë™ì˜ìƒ API í˜¸ì¶œ ì™„ë£Œ ({api_time:.2f}ì´ˆ) - {len(popular_videos)}ê°œ ì˜ìƒ")

        # 3. í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ Vector DBì— ì €ì¥
        for popular in popular_videos:
            pop_video_text = (
                f"ì œëª©(ê°€ì¤‘ì¹˜ ë†’ìŒ): {popular['video_title']}.\n"
                f"ì£¼ìš” íƒœê·¸: {popular['video_hash_tag']}.\n"
                f"ì˜ìƒ ì„¤ëª…: {popular['video_description'][:500]}"  # ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ
            )
            await self.content_chunk_repository.save_context(
                source_type=SourceTypeEnum.IDEA_RECOMMENDATION,
                source_id=int(category.value),
                context=pop_video_text)


    async def analyze_idea(self, idea_req: IdeaRequest, channel: Channel, summary: str) -> List[Dict[str, Any]]:
        try:
            # 1. ë‚´ ì±„ë„ ì •ë³´ + ìš”ì²­ ë‚´ìš©
            origin_context = f"""
- ì±„ë„ëª…: {channel.name}
- ì±„ë„ ì»¨ì…‰: {channel.concept}
- íƒ€ê²Ÿ ì‹œì²­ì: {channel.target}
- ì¹´í…Œê³ ë¦¬ : {channel.channel_hash_tag.name}
- ìµœê·¼ ì˜ìƒì˜ í•µì‹¬ ë‚´ìš©: {summary}
            """
            logging.info("ì•„ì´ë””ì–´ ë‚´ ì±„ë„ í™•ì¸ : %s", origin_context)

            request_context = []

            if idea_req.keyword:
                request_context.append(f"- ì•„ì´ë””ì–´ í‚¤ì›Œë“œ : {idea_req.keyword}")
            if idea_req.detail:
                request_context.append(f"- ì•„ì´ë””ì–´ ì„¤ëª… : {idea_req.detail}")
            if idea_req.video_type:
                request_context.append(f"- ì•„ì´ë””ì–´ ì˜ìƒ ìœ í˜• : {idea_req.video_type}")

            request_context = "\n".join(request_context) if request_context else ""


            # 2. ì˜ìƒê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ 'ì¸ê¸° ì˜ìƒ' ì²­í¬ë¥¼ ê²€ìƒ‰ (Vector DB)
            search_start = time.time()
            logger.info("ğŸ” ìœ ì‚¬ ì¸ê¸° ì˜ìƒ ë²¡í„° ê²€ìƒ‰ ì¤‘...")
            query_text = f"ì»¨ì…‰: {channel.concept}, ì¹´í…Œê³ ë¦¬: {channel.channel_hash_tag}, ìµœê·¼ ì˜ìƒ ìš”ì•½: {summary}"

            video_embedding = await self.content_chunk_repository.generate_embedding(query_text)
            meta_data = {"query_embedding": str(video_embedding), "source_id": int(channel.channel_hash_tag.value)}
            similar_chunks = await self.content_chunk_repository.search_similar_by_embedding(
                SourceTypeEnum.IDEA_RECOMMENDATION, metadata=meta_data, limit=5
            )

            search_time = time.time() - search_start
            logger.info(f"ğŸ” ìœ ì‚¬ ì¸ê¸° ì˜ìƒ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ ({search_time:.2f}ì´ˆ) - {len(similar_chunks)}ê°œ ì²­í¬")

            # 3. ê²€ìƒ‰ëœ ì²­í¬(ë‚´ìš©)ë¥¼ í…ìŠ¤íŠ¸ë¡œ (í† í° íš¨ìœ¨ì„±ì„ ìœ„í•´ 'ì œëª©'ë§Œ ì¶”ì¶œí•˜ê±°ë‚˜, ì €ì¥ëœ contentë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            popularity_context = "\n".join([chunk.get("content", "") for chunk in similar_chunks])

            input_data = {
                "request": request_context,
                "origin": origin_context,
                "popularity": popularity_context
            }
            full_prompt = PromptTemplateManager.get_idea_prompt(input_data)
            logger.info("ğŸ¤– ì•„ì´ë””ì–´ ìƒì„± - LLM í˜¸ì¶œ ì „ ì „ì²´ í”„ë¡¬í”„íŠ¸:\n%s", full_prompt)

            # 4. LLM ì‹¤í–‰
            llm_start = time.time()
            logger.info("ğŸ¤– ì•„ì´ë””ì–´ ìƒì„± LLM ì‹¤í–‰ ì¤‘...")

            result_str = await self.llm.ainvoke(full_prompt)

            llm_time = time.time() - llm_start
            logger.info(f"ğŸ¤– ì•„ì´ë””ì–´ ìƒì„± LLM ì‹¤í–‰ ì™„ë£Œ ({llm_time:.2f}ì´ˆ)")

            # LLMì˜ ì‘ë‹µ ë¬¸ìì—´ì„ JSON íŒŒì‹±
            clean_json_str = result_str.content.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_json_str)
        except Exception as e:
            logger.error(f"ì•„ì´ë””ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e!r}")
            raise e

    
    async def analyze_algorithm_optimization(self, video_id: str, skip_vector_save: bool = False) -> str:
        """
        ìœ íŠœë¸Œ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„
        
        Args:
            video_id: YouTube ì˜ìƒ ID
            
        Returns:
            ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ê²°ê³¼
        """
        try:
            # ì˜ìƒ ìƒì„¸ ì •ë³´ ì¡°íšŒ (YouTube API + Redis ìºì‹±)
            video_start = time.time()
            logger.info("ğŸ“¹ YouTube ì˜ìƒ ìƒì„¸ ì •ë³´ API í˜¸ì¶œ ì¤‘...")
            video_details = await self.video_detail_service.get_video_details(video_id)
            video_time = time.time() - video_start
            logger.info(f"ğŸ“¹ YouTube ì˜ìƒ ìƒì„¸ ì •ë³´ API í˜¸ì¶œ ì™„ë£Œ ({video_time:.2f}ì´ˆ)")

            # ì±„ë„ ì •ë³´ ì¡°íšŒ (YouTube API)
            channel_id = video_details.get('channelId')

            channel_stats = {}
            if channel_id:
                channel_start = time.time()
                logger.info("ğŸ“º YouTube ì±„ë„ í†µê³„ API í˜¸ì¶œ ì¤‘...")
                channel_stats = self.video_detail_service.get_channel_stats(channel_id)
                channel_time = time.time() - channel_start
                logger.info(f"ğŸ“º YouTube ì±„ë„ í†µê³„ API í˜¸ì¶œ ì™„ë£Œ ({channel_time:.2f}ì´ˆ)")
            
            # ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„° êµ¬ì¡°í™”
            optimization_data = {
                "video": {
                    "title": video_details.get('title', ''),
                    "description": video_details.get('description', ''),
                    "tags": video_details.get('tags', []),
                    "publishedAt": video_details.get('publishedAt', ''),
                    "duration": video_details.get('duration', ''),
                    "viewCount": video_details.get('viewCount', 0),
                    "likeCount": video_details.get('likeCount', 0),
                    "commentCount": video_details.get('commentCount', 0),
                    "thumbnails": video_details.get('thumbnails', {})
                },
                "channel": {
                    "name": video_details.get('channelTitle', ''),
                    "subscriberCount": channel_stats.get('subscriberCount', 0),
                    "totalViewCount": channel_stats.get('viewCount', 0),
                    "totalVideoCount": channel_stats.get('videoCount', 0)
                }
            }
            
            # JSON í˜•ì‹ìœ¼ë¡œ context ìƒì„±
            context = json.dumps(optimization_data, ensure_ascii=False, indent=2)
            
            # ìœ ì‚¬í•œ ì´ì „ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ì‚¬ë¡€ ê²€ìƒ‰ (skip_vector_saveê°€ Falseì¸ ê²½ìš°ë§Œ)
            if not skip_vector_save:
                query_text = f"ì œëª©: {video_details.get('title', '')}, ì„¤ëª…: {video_details.get('description', '')[:200]}"
                similar_chunks = await self.content_chunk_repository.search_similar_optimization(
                    query_text=query_text,
                    limit=3
                )
                
                # ì´ì „ ë¶„ì„ ì‚¬ë¡€ê°€ ìˆìœ¼ë©´ contextì— ì¶”ê°€
                if similar_chunks:
                    previous_cases = "\n\n---\n\n".join([chunk.get("content", "") for chunk in similar_chunks])
                    context += f"\n\n## ìœ ì‚¬ ì˜ìƒì˜ ì´ì „ ìµœì í™” ë¶„ì„ ì‚¬ë¡€:\n{previous_cases}"
            
            query = "ì´ ìœ íŠœë¸Œ ì˜ìƒì˜ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ìƒíƒœë¥¼ ë¶„ì„í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”."
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸° ë° LLM ì‹¤í–‰
            llm_start = time.time()
            prompt_template = PromptTemplateManager.get_algorithm_optimization_prompt()
            result = self.execute_llm_chain(context, query, prompt_template)
            llm_time = time.time() - llm_start
            logger.info(f"ğŸ¤– ì•Œê³ ë¦¬ì¦˜ ìµœì í™” LLM ì‹¤í–‰ ì™„ë£Œ ({llm_time:.2f}ì´ˆ)")
            
            return result
        
        except Exception as e:
            logger.error(f"ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e
            

    def analyze_realtime_trends(self, limit: int = 5, geo: str = "KR") -> Dict:
        """
        ì‹¤ì‹œê°„ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ì—¬ YouTube ì½˜í…ì¸ ì— ì í•©í•œ í˜•íƒœë¡œ ë°˜í™˜
        
        Args:
            limit: ë¶„ì„í•  íŠ¸ë Œë“œ ê°œìˆ˜ (ìµœëŒ€ 5ê°œ)
            geo: ì§€ì—­ ì½”ë“œ (ê¸°ë³¸ê°’: KR)
            
        Returns:
            ë¶„ì„ëœ íŠ¸ë Œë“œ ì •ë³´
        """
        # 1. Google Trendsì—ì„œ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸° (Google Trends API)
        trends_start = time.time()
        logger.info("ğŸ“ˆ Google Trends ì‹¤ì‹œê°„ íŠ¸ë Œë“œ API í˜¸ì¶œ ì¤‘...")
        raw_trends = self.trend_service.get_realtime_trends(limit=limit*2, geo=geo)  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜¤ê¸°
        trends_time = time.time() - trends_start
        logger.info(f"ğŸ“ˆ Google Trends ì‹¤ì‹œê°„ íŠ¸ë Œë“œ API í˜¸ì¶œ ì™„ë£Œ ({trends_time:.2f}ì´ˆ) - {len(raw_trends) if raw_trends else 0}ê°œ íŠ¸ë Œë“œ")
        
        if not raw_trends:
            return {"error": "íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        
        # 3. Context êµ¬ì„±
        context = {
            "trends_data": raw_trends,
            "current_date": current_date,
            "region": geo
        }
        
        # 4. LLMì—ê²Œ ë¶„ì„ ìš”ì²­
        query = f"ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ì¤‘ YouTube ì½˜í…ì¸ ë¡œ ì í•©í•œ ìƒìœ„ {limit}ê°œë¥¼ ì„ ì •í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt_template = PromptTemplateManager.get_trend_analysis_prompt()
        
        # 5. LLM ì‹¤í–‰ ë° ê²°ê³¼ íŒŒì‹±
        llm_start = time.time()
        logger.info("ğŸ¤– ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ LLM ì‹¤í–‰ ì¤‘...")
        result_str = self.execute_llm_chain(
            context=json.dumps(context, ensure_ascii=False),
            query=query,
            prompt_template_str=prompt_template
        )
        llm_time = time.time() - llm_start
        logger.info(f"ğŸ¤– ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ LLM ì‹¤í–‰ ì™„ë£Œ ({llm_time:.2f}ì´ˆ)")
        
        try:
            clean_json_str = result_str.strip().replace("```json", "").replace("```", "")
            result = json.loads(clean_json_str)
            return result
        except json.JSONDecodeError:
            return {"error": "ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜", "raw_result": result_str}
    
    

    def analyze_channel_trends(
        self,
        channel_concept: str,
        target_audience: str,
        latest_trend_keywords : List[TrendKeyword]

    ) -> Dict:
        """
        ì±„ë„ ë§ì¶¤í˜• íŠ¸ë Œë“œë¥¼ ìƒì„±í•˜ê³  ë¶„ì„
        
        Args:
            channel_concept: ì±„ë„ ì»¨ì…‰
            target_audience: íƒ€ê²Ÿ ì‹œì²­ì
            
        Returns:
            ì±„ë„ ë§ì¶¤í˜• íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
        """
        # 1. Context êµ¬ì„± (ì±„ë„ ì •ë³´)
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        
        # TrendKeyword ë¦¬ìŠ¤íŠ¸ â†’ dictë¡œ ë³€í™˜
        real_time_keywords_data = [
            {
                "keyword": kw.keyword,
                "score": kw.score,
                "created_at": kw.created_at.strftime("%Y-%m-%d %H:%M:%S")  # datetime â†’ str
            }
            for kw in latest_trend_keywords
]
        context = {
            "channel_concept": channel_concept,
            "target_audience": target_audience,
            "current_date": current_date,
            "latest_5_trend_keywords": real_time_keywords_data

        }
        
        # 2. LLMì—ê²Œ ë¶„ì„ ìš”ì²­
        query = "ì±„ë„ì— ìµœì í™”ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ 5ê°œë¥¼ ìƒì„±í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”."
        prompt_template = PromptTemplateManager.get_channel_customized_trend_prompt()
        
        # 3. ì±„ë„ ë§ì¶¤í˜• íŠ¸ë Œë“œë¥¼ ìœ„í•œ íŠ¹ë³„ ì²˜ë¦¬
        documents = [Document(page_content=json.dumps(context, ensure_ascii=False))]
        
        # í•„ìš”í•œ ëª¨ë“  ë³€ìˆ˜ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt = PromptTemplate(
            input_variables=["input", "context", "channel_concept", "target_audience", "current_date"],
            template=prompt_template
        )
        
        chat_prompt = ChatPromptTemplate.from_messages([
            HumanMessagePromptTemplate(prompt=prompt)
        ])
        
        # ì²´ì¸ ì‹¤í–‰
        llm_start = time.time()
        logger.info("ğŸ¤– ì±„ë„ ë§ì¶¤í˜• íŠ¸ë Œë“œ ë¶„ì„ LLM ì‹¤í–‰ ì¤‘...")
        combine_chain = create_stuff_documents_chain(self.llm, chat_prompt)
        result_str = combine_chain.invoke({
            "input": query,
            "context": documents,
            "channel_concept": channel_concept,
            "target_audience": target_audience,
            "current_date": current_date,
            "latest_5_trend_keywords": real_time_keywords_data

        })
        llm_time = time.time() - llm_start
        logger.info(f"ğŸ¤– ì±„ë„ ë§ì¶¤í˜• íŠ¸ë Œë“œ ë¶„ì„ LLM ì‹¤í–‰ ì™„ë£Œ ({llm_time:.2f}ì´ˆ)")
        
        
        try:
            clean_json_str = result_str.strip().replace("```json", "").replace("```", "")
            result = json.loads(clean_json_str)
            return result
        except json.JSONDecodeError:
            return {"error": "ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜", "raw_result": result_str}



    def execute_llm_chain(self, context: str, query: str, prompt_template_str: str) -> str:
        """
        LLM ì²´ì¸ì„ ì‹¤í–‰í•˜ëŠ” ê³µí†µ ë©”ì„œë“œ
        :param context: LLMì— ì œê³µí•  ì •ë³´(youtube apië¥¼ í†µí•´ ê°€ì ¸ì˜¨ ìë§‰ ë“±)
        :param query: ì‚¬ìš©ì ì§ˆë¬¸
        :return: LLMì˜ ì‘ë‹µ
        """
        documents = [Document(page_content=context)]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt_template = PromptTemplate(
            input_variables=["input", "context"],
            template=prompt_template_str
        )

        chat_prompt = ChatPromptTemplate.from_messages([
            HumanMessagePromptTemplate(prompt=prompt_template)
        ])

        # ì²´ì¸ ì¡°í•© ë° ì‹¤í–‰
        combine_chain = create_stuff_documents_chain(self.llm, chat_prompt)
        result = combine_chain.invoke({"input": query, "context": documents})
        return result
    
    def execute_llm_direct(self, prompt: str) -> str:
        """
        ì´ë¯¸ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ë°”ë¡œ LLMì— ë„£ì–´ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜

        :param prompt: ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        :return: LLMì˜ ì‘ë‹µ
        """
        # self.llmì´ ì§ì ‘ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ë°›ì•„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ë¼ê³  ê°€ì •
        result = self.llm.invoke(prompt)
        return result.content

    async def create_update_summary(self, prev_report: ReportLog, curr_report: Report):
        """
        ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸ ì‹œ ë³€ê²½ì  ìš”ì•½ ìƒì„±
        """

        try:
            # 1. ë°ì´í„° ê°€ê³µ í—¬í¼ í•¨ìˆ˜ (ë‚´ë¶€ ì •ì˜)
            def safe_get(val, default=0):
                return val if val is not None else default

            def summarize_text(text):
                return text[:100] + "..." if text and len(text) > 100 else (text or "ë‚´ìš© ì—†ìŒ")

            def calc_diff_msg(val, avg):
                if val is None or avg is None or avg == 0:
                    return "ì •ë³´ ì—†ìŒ"
                diff = val - avg
                return f"{'+' if diff > 0 else ''}{int(diff)}"

            # 2. í…œí”Œë¦¿ì— ì „ë‹¬í•  ë°ì´í„° ì¤€ë¹„ (dict ë³€í™˜)
            template_data = {
                "title": curr_report.title,

                # ì´ì „ ë°ì´í„°
                "prev_view": safe_get(prev_report.view),
                "prev_view_diff": calc_diff_msg(prev_report.view, prev_report.view_channel_avg),
                "prev_like": safe_get(prev_report.like_count),
                "prev_comment": safe_get(prev_report.comment),
                "prev_pos": safe_get(prev_report.positive_comment),
                "prev_neg": safe_get(prev_report.negative_comment),
                "prev_concept": safe_get(prev_report.concept),
                "prev_seo": safe_get(prev_report.seo),
                "prev_revisit": safe_get(prev_report.revisit),
                "prev_leave": summarize_text(prev_report.leave_analyze),

                # í˜„ì¬ ë°ì´í„°
                "curr_view": safe_get(curr_report.view),
                "curr_view_diff": calc_diff_msg(curr_report.view, curr_report.view_channel_avg),
                "curr_like": safe_get(curr_report.like_count),
                "curr_comment": safe_get(curr_report.comment),
                "curr_pos": safe_get(curr_report.positive_comment),
                "curr_neg": safe_get(curr_report.negative_comment),
                "curr_concept": safe_get(curr_report.concept),
                "curr_seo": safe_get(curr_report.seo),
                "curr_revisit": safe_get(curr_report.revisit),
                "curr_leave": summarize_text(curr_report.leave_analyze),
            }

            # íƒ¬í”Œë¦¿ ìƒì„±
            update_summary_prompt = PromptTemplateManager.summarize_update_changes(template_data)

            # 4. LLM ì‹¤í–‰
            llm_start = time.time()
            logger.info("ğŸ¤– ì—…ë°ì´íŠ¸ ìš”ì•½ LLM ì‹¤í–‰ ì¤‘...")

            response = await self.llm.ainvoke(update_summary_prompt)
            update_summary_text = response.content # ê°ì²´ì—ì„œ ë¬¸ìì—´ ì¶”ì¶œ

            llm_time = time.time() - llm_start
            logger.info(f"ğŸ¤– ì—…ë°ì´íŠ¸ ìš”ì•½ LLM ì‹¤í–‰ ì™„ë£Œ ({llm_time:.2f}ì´ˆ)")

            return update_summary_text

        except Exception as e:
            logger.error(f"âŒ ì—…ë°ì´íŠ¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            raise e